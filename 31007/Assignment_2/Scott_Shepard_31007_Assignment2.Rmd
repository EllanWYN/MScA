---
title: 'Week 2: Assignment'
author: "Scott Shepard"
date: "4/4/2018"
output:
  pdf_document: default
  html_document: default
subtitle: MScA, Statistical Analysis (31007)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Generate uniformly distributed random numbers

## 1.1 Use runif()

Function runif(N,a,b) simulates N pseudo-random numbers uniformly distributed on [a,b].

```{r, 1.1}
set.seed(15)
Sample <- runif(1000, 0, 1)
```

## 1.2 Simulate Uniform Random Sample on [0,1] Using Random.org.

```{r, 1.2}
library(random)

nFlips <- 1000

dataFromRandom <- randomNumbers(n=nFlips, min=0, max=1, col=1, base=2, check=TRUE)
head(dataFromRandom)

```

## 1.4 Turning binary sequence to uniform random numbers

Turn your sequence of {0,1} into uniform random numbers on [0,1].

Create function that turns a sequence of zeros and ones of length n into decimal form.

```{r, 1.4.1}
bitsToInt <- function(x) {
    packBits(rev(c(rep(FALSE, 32-length(x)%%32), as.logical(x))), "integer")
}
bitsToInt(c(1,1,1,1,1,0))
```

Turn the sequence ofzeros and ones dataFromRandom of length 1000 into a matrix with 10 columns and 100 rows

```{r, 1.4.2}
Binary.matrix <- matrix(dataFromRandom,ncol=10)
head(Binary.matrix)
```

Transform each row of the matrix into decimal format using `bin2dec()` and divide the numbers by 2^10 to make real numbers in [0,1].

```{r, 1.4.3}
dataFromRandom.dec <- apply(Binary.matrix,1,bitsToInt)/2^10
head(dataFromRandom.dec)
```

# 2 Test random number generators

## 2.1 Test uniformity of distribution of both random number generators

### 2.1.1 Using Sample generated by runif()

Analyze what was simulated by first looking at the histogram.

```{r, 2.1.1-1}
Sample.histogram <- hist(Sample)
```

```{r, 2.1.1-2}
Sample.histogram
```

**What does the histogram tell you about the distribution? Is it consistent with the goal of simulation?**

Estimate mean and standard deviation of Sample.histogram$density.

```{r, 2.1.1-3}
(Sample.histogram.mean <- mean(Sample.histogram$density))
```

```{r, 2.1.1-4}
(Sample.histogram.sd<-sd(Sample.histogram$density))
```

```{r, 2.1.1-5}
plot(Sample.histogram,freq=FALSE,ylim=c(0,Sample.histogram.mean+2*Sample.histogram.sd))
abline(h=Sample.histogram.mean)
abline(h=Sample.histogram.mean+1.96*Sample.histogram.sd,col="red",lty=2)
abline(h=Sample.histogram.mean-1.96*Sample.histogram.sd,col="red",lty=2)
```

**What does the graph tell you about the observed distribution?**

Estimate moments of Sample.

```{r, 2.1.1-6}
(Sample.mean<-mean(Sample))
```

```{r, 2.1.1-7}
(Sample.variance<-var(Sample))
```

**What do you conclude about the estimated distribution from the moments?**

Check the summary of the simulated sample.

```{r, 2.1.1-8}
summary(Sample)
```

**What do you think is the best way of estimating uniform distribution over unknown interval?**

### 2.1.2 Repeat the same steps to test uniformity of the sample from Random.org

```{r, 2.1.2-1}
Sample.histogram<-hist(dataFromRandom.dec)
```

```{r, 2.1.2-2}
Sample.histogram
```

```{r, 2.1.2-3}
(Sample.histogram.mean<-mean(Sample.histogram$density))
```

```{r, 2.1.2-4}
(Sample.histogram.sd<-sd(Sample.histogram$density))
```

```{r, 2.1.2-5}
plot(Sample.histogram,freq=FALSE,ylim=c(0,Sample.histogram.mean+2*Sample.histogram.sd))
abline(h=Sample.histogram.mean)
abline(h=Sample.histogram.mean+1.96*Sample.histogram.sd,col="red",lty=2)
abline(h=Sample.histogram.mean-1.96*Sample.histogram.sd,col="red",lty=2)
```

```{r, 2.1.2-6}
(Sample.mean<-mean(dataFromRandom.dec))
```

```{r, 2.1.2-7}
(Sample.variance<-var(dataFromRandom.dec))
```

```{r, 2.1.2-8}
summary(dataFromRandom.dec)
```

## 2.2 Test independence of the sequence of zeros and ones

### 2.2.1 Turning point test

Turning point test is used to check if a sequence of numbers is i.i.d. (independent identically distributed).
The test is based on the number of turning points in the sequence.
The number of turning points is the number of maxima and minima in the series.
Let \(T\) be the number of turning points in a sample of length \(n\) large enough.
Then the statistic of the test \[z=\frac{T-\frac{2n-4}{3}}{\sqrt{\frac{16n-29}{90}}}\] has standard normal distribution.

The test is performed by turning.point.test() in package randtests

```{r}
library(randtests)
```

```{r}
turning.point.test(dataFromRandom.dec)
```

The null hypothesis tested by turning point test is randomness (i.i.d.). The alternative is serial correlation in the sequence. Thus, if the test returns a very small p-value the randomness needs to be rejected.

The p-value here is quite high so the randomness cannot be rejected.

### 2.2.2 Test frequency by Monobit test

To perform Monobit test you need to transform your {0,1} sample into {-1,1}.
Illustrate the test on the sequence simulated in the previous lecture.

We created the sequence of coin tosses:

```{r}
dataFromRandom.plusminus1<-(dataFromRandom-.5)*2
```

Recall from the lecture notes that monobit test of randomness is based on the statistic

\[S=\frac{|\sum_{i=1}^{N}R_i|}{\sqrt{2N}} \sim erfc,\]

where \(R_i\) is the i-th random number, summation is done over all \(N=nFlips\) random numbers.

erfc is the complimentary error function, a special function complimentary to error function erf=1-erfc.

Both functions can be easily calculated in R with the help of pnorm:

```{r}
erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)
```

```{r}
plot(seq(from=-3,to=3,by=.05),erfc(seq(from=-3,to=3,by=.05)),type="l",xlab="x",ylab="erfc(x")
```

To test the sequence \(R_i\) check the value erfc(S).

If the P-value or erfc(S) is less or equal than 0.01 the sequence fails the test.

```{r}
erfc(abs(sum(dataFromRandom.plusminus1)/sqrt(2*nFlips)))
```

The test shows that the Random.org sequence passes.

Now check each of the sub-sequences created earlier:

```{r}
plot(erfc(abs(apply(matrix(dataFromRandom.plusminus1,ncol=50),1,sum))/sqrt(2*50)),ylab="P-values of 20 runs")
abline(h=.01)
```
How many runs out of 20 fail the test?

```{r}
sum(erfc(abs(apply(matrix(dataFromRandom.plusminus1,ncol=50),1,sum))/sqrt(2*50))<=.01)
```

# 3 Invent a random number generator

For my source of "random" numbers I used Lake Michigan water temperature
measures at Montrose harbor from 2013

```{r}
suppressWarnings(library(dplyr))

df <- read.csv("~/Dropbox/MScA/31007 - Stats Analysis/Assignment 2/Beach_Water_Quality_-_Automated_Sensors.csv", stringsAsFactors = F)

df$Measurement.Timestamp <- strptime(df$Measurement.Timestamp, "%m/%d/%Y %H:%M:%S %p")
df <- df[df$Measurement.Timestamp > '2014-05-31', ]
df <- df[df$Measurement.Timestamp < '2014-07-01', ]
df <- df[df$Beach.Name == 'Montrose Beach', ]
sequence <- df$Water.Temperature
sequence <- sequence[!is.na(sequence)]
```

### Uniformity Test

```{r}
sequence.hist <- hist(sequence)
```

```{r}
(sequence.hist.mean <- mean(sequence.hist$density))
```

```{r}
(sequence.hist.sd <- sd(sequence.hist$density))
````

```{r}
plot(sequence.hist,freq=FALSE,ylim=c(0,sequence.hist.mean+2*sequence.hist.sd))
abline(h=sequence.hist.mean)
abline(h=sequence.hist.mean+1.96*sequence.hist.sd,col="red",lty=2)
abline(h=sequence.hist.mean-1.96*sequence.hist.sd,col="red",lty=2)
```

```{r}
(sequence.hist.var  <- var(sequence.hist$density))
```

```{r}
plot(df$Measurement.Timestamp, df$Water.Temperature)
```

### Turning Point Test
```{r}
suppressWarnings(library(randtests))
turning.point.test(sequence)
```

### Monobit Test
```{r}
dataFromRandom.plusminus1 <- (sequence-.5)*2

erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)

erfc(abs(sum(dataFromRandom.plusminus1)/sqrt(2*length(sequence))))
```

# 4 Monte Carlo Method

## 4.1 Scratch off quote of the day: fuction download

Download function `ScratchOffMonteCarlo()` contained in a binary file `ScratchOffMonteCarlo.rda` from the web site, put it in a folder with path

```{r}
load("ScratchOffMonteCarlo.rda")
```

## 4.2 Simulate pseudo-random poins \([x,y]\) on \([0,100] \times [0,100]\)


Select a number o points nSample.

Simulate a sample of length 2*nSample from uniform distribution on [0,100] and turn it into a \((nSample \times 2)\) matrix.
Use a seed of your choice my.seed

Throw nSample simulated points on square \([0,100] \times [0,100]\) to scratch off some of yellow paint.

```{r}
nSample <-21000
my.seed <- 15101
set.seed(my.seed)
xy<-runif(2*nSample,0,100)
xy<-matrix(xy,ncol=2)
head(xy)
```

```{r}
ScratchOffMonteCarlo(xy)
```

Take a note of the percentage scratched off returned by ScratchOffMonteCarlo(xy).

**By changing nSample and my.seed try to make the quote of the day readable with minimum sample size.**
**What percent you needed to scratch off to make the quote readable?**

I needed to scratch off about 93% of the yellow to make the quote readable.

"The purpose of models is not to fir the data but to sharpen the questions
within" - Samuel Karlin

I started with the seed 151. I needed a sample of 28000 to read the quote.  
I then tried to play around the the seed with a lower sample of only 20000 to 
see if I could read the quote at any point. It didn't seem like changing the 
seed changed the percentage of the chart that was shown. It always stayed 
around 88% displayed.

## 4.3 Simulate quasi-random poins \([x,y]\) on \([0,100] \times [0,100]\)

function runif() can be replaced by sobol() from library randtoolbox.

```{r}
suppressWarnings(library(randtoolbox))
```

```{r}
my.seed<-151
set.seed(my.seed)
nSample<-10
xy<-sobol(nSample,dim=2,init=T)*100
```

Then make init=F if you want to generate different sample every time or keep it equal to T if you want repeated samples.

```{r}
nSample<-17000
xy<-sobol(nSample,dim=2,init=F,scrambling = T,seed=my.seed)*100

plot(xy)
```

```{r}
ScratchOffMonteCarlo(xy)
```

**Again, by changing nSample and my.seed try to make the quote of the day readable with minimum sample size.**

At my.seed = 10 I needed an nSample of 20000 to get to 93% scratch off.  
I changed my.seed to 151 and found I only needed a sample of 15000 to get the 
93% scratch off. I found this was the lowest I could get.

**What percent you needed to scratch off to make the quote readable?**

I stil need the same 93% scratch off to view the quote as I did with the other
Monte Carlo method.

**Which of the Monte Carlo methods makes the quote readable sooner?**

The quasi-random sobol method definitely makes the quote readable faster.

**Which parameters nSample and my.seed gave you the best result, what percent of the yellow paing you were able to scratch off by each method?**

I found the pairing my.seed = 151 & nSample = 17000 gave me the best results.

**Changing which of the two parameters plays more significant role?**

Changing nSample gave me, by far, the greatest impact. Changing my.seed has 
some impact but now much and there was no clear pattern.

# 5 Test

Download your uniform(0,1) sample from left sidebar, unpack and read it.

Create variable dataPath' equal to path to your local folder where you saved the data fileWeek2_Test_Sample.csv`.
It should look like:

```{r}
dat <- read.csv('Week2_Test_Sample.csv', header=TRUE)$x
```

The sample dat has the following format:

- dat[1] - mean value of normal distribution;
- dat[2] - standard deviation of normal distribution;
- dat[3] - intensity of exponential distribution;
- dat[4]:dat[504] sample from uniform distribution on [0.1].

Using this sample, create:

- Sample datNorm from normal distribution with mean dat[1] and standard deviation dat[2];
- Sample datExp from exponential distribution with intensity dat[3].

```{r}
datNorm <- qnorm(dat[4:503], dat[1], dat[2])
datExp <- qexp(dat[4:503], dat[3])

res<-cbind(datNorm=datNorm,datExp=datExp)
```

```{r}
write.csv(res, 'result.csv', row.names = F)
```
