---
title: "Course Assignment"
subtitle: "31007: Statistical Analysis"
author: "Scott Shepard"
date: "5/8/2018"
output: pdf_document
---

Completion:  
  - [x] Step 1  
  - [x] Step 2  
  - [x] Step 3  
  - [x] Step 4  
  - [x] Step 5  
  - [ ] Step 6  
  - [ ] Step 7  

```{r include=FALSE}
suppressWarnings(library(dplyr))
library(ggplot2)
```

## Step 1.

Read the data and visualize and get familiar with the variables.

```{r}
datapath <- "~/Dropbox/MScA/31007 - Stats Analysis/Course Project"

AssignmentData <- read.csv(
  file=file.path(datapath, "RegressionAssignmentData2014.csv"),
  row.names=1,
  header=TRUE,
  sep=",")

head(AssignmentData)
```

The first 7 variables (input variables) are the daily records of the US Treasury yields to maturity. The meaning of the variable Output will become clear later.

Plot the input variables.

```{r}
matplot(AssignmentData[,-c(8,9,10)], type='l')
```

Plot the input variables together with the output variable.

```{r}
matplot(AssignmentData[,-c(9,10)], type='l')
```

## Step 2

Estimate simple regression model with each of the input variables and the output variable given in AssignmentData.

### Three Month Bill

```{r}
m3M <- lm(Output1 ~ USGG3M, data=AssignmentData)
```

Check available names of fields returned by lm() and summary() functions

```{r names}
(names(m3M))
(names(summary(m3M)))
```

Analyze the summary

```{r summary_3_mo_model}
summary(m3M)
```

This variable, `USGG3M`, which trackes the rates of three month treasury bonds, 
is quite a good predictor of `Output1`. The p-value indicates a statistically 
significant slope. The adjusted R-squared is at 96% which is really high.

The key value of 2.5 for the estimated slope indicates that when all 

```{r}
c(Total.Variance=var(AssignmentData[,8]), Unexplained.Variance=summary(m3M)$sigma^2)
```

There remains only a small portion of the total variance that is unexplained.
The ratio of these two numbers gives the unadjusted R^2 in summary.

```{r}
matplot(AssignmentData[,8],type="l", xaxt="n")
lines(m3M$fitted.values, col="red")
```

### Six Month Bill

```{r}
m6M <- lm(Output1 ~ USGG6M, data=AssignmentData)

summary(m6M)
```

The six-month yield input does an even better job of predicting `Output1` than
the three-month yield did, explaining 97.4% of the variance.

```{r}
c(Total.Variance=var(AssignmentData[,8]), Unexplained.Variance=summary(m6M)$sigma^2)
```

There is less unexplained variance with this input variable, which matches
the R^2 number in summary. The intercept is slightly different but the 
slope is about the same.

```{r}
matplot(AssignmentData[,8], type="l",xaxt="n")
lines(m6M$fitted.values, col="red")
```

### Two Year Note

```{r}
m2Y <- lm(Output1 ~ USGG2YR, data=AssignmentData)

summary(m2Y)
```

```{r}
c(Total.Variance=var(AssignmentData[,8]), Unexplained.Variance=summary(m2Y)$sigma^2)
```

```{r}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(m2Y$fitted.values,col="red")
```

### Three Year Note

```{r}
m3Y <- lm(Output1 ~ USGG3YR, data=AssignmentData)

summary(m3Y)
```

```{r}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(m3Y)$sigma^2)
```

```{r}
matplot(AssignmentData[,8],type="l", xaxt="n")
lines(m3Y$fitted.values, col="red")
```

### Five Year Note

```{r}
m5Y <- lm(Output1 ~ USGG5YR, data=AssignmentData)

summary(m5Y)
```

```{r}
c(Total.Variance=var(AssignmentData[,8]), Unexplained.Variance=summary(m5Y)$sigma^2)
```

```{r}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(m5Y$fitted.values,col="red")
```

### Ten Year Note
 
```{r}
m10 <- lm(Output1 ~ USGG10YR, data=AssignmentData)

summary(m10)
```

```{r}
c(Total.Variance=var(AssignmentData[,8]), Unexplained.Variance=summary(m10)$sigma^2)
```

```{r}
matplot(AssignmentData[,8], type="l", xaxt="n")
lines(m10$fitted.values, col="red")
```

### Thirty Year Note

```{r}
m30 <- lm(Output1 ~ USGG30YR, data=AssignmentData)

summary(m30)
```

```{r}
c(Total.Variance=var(AssignmentData[,8]), Unexplained.Variance=summary(m30)$sigma^2)
```

```{r}
matplot(AssignmentData[,8], type="l", xaxt="n")
lines(m30$fitted.values, col="red")
```

### All Together Now

Collect all slopes and intercepts in one table and print this table. Try to do it in one line using apply() function.

```{r}
Output1 <- AssignmentData$Output1
t(apply(AssignmentData[,-c(8:10)], 2, function(x) lm(Output1~x)$coef))
```

# Step 3

Fit linear regression models using single output (column 8 Output1) as input and each of the original inputs as outputs.

Collect all slopes and intercepts in one table and print this table.
```{r}
t(apply(AssignmentData[,-c(8:10)], 2, function(y) lm(y~Output1)$coef))
```

# Step 4

Estimate logistic regression using all inputs and the data on FED tightening and easing cycles.

```{r}
AssignmentDataLogistic <- data.frame(data.matrix(AssignmentData, rownames.force="automatic"))
```

Prepare the easing-tightening data.
Make the easing column equal to 0 during the easing periods and NA otherwise.
Make the tightening column equal to 1 during the tightening periods and NA otherwise.

```{r}
# Create columns of easing periods (as 0s) and tightening periods (as 1s)
EasingPeriods<-AssignmentDataLogistic[,9]
EasingPeriods[AssignmentDataLogistic[,9]==1]<-0
TighteningPeriods<-AssignmentDataLogistic[,10]
# Check easing and tightening periods
cbind(EasingPeriods, TighteningPeriods)[c(550:560,900:910,970:980),]
```

Remove the periods of neither easing nor tightening.

```{r}
All.NAs <- is.na(EasingPeriods)&is.na(TighteningPeriods)
AssignmentDataLogistic.EasingTighteningOnly <- AssignmentDataLogistic
AssignmentDataLogistic.EasingTighteningOnly[,9] <- EasingPeriods
AssignmentDataLogistic.EasingTighteningOnly <- AssignmentDataLogistic.EasingTighteningOnly[!All.NAs,]
AssignmentDataLogistic.EasingTighteningOnly[is.na(AssignmentDataLogistic.EasingTighteningOnly[,10]),10] <- 0
# Binary output for logistic regression is now in column 10
```

Plot the data and the binary output variable representing easing (0) and tightening (1) periods.

```{r}
matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)], type="l", ylab="Data and Binary Fed Mode")
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20, col="red")
```

Estimate logistic regression with 3M yields as predictors for easing/tightening output.

```{r}
LogisticModel.TighteningEasing_3M <- 
  glm(AssignmentDataLogistic.EasingTighteningOnly[,10] ~
        AssignmentDataLogistic.EasingTighteningOnly[,1], 
      family=binomial(link=logit))
summary(LogisticModel.TighteningEasing_3M)
```

```{r}
matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)], type="l", ylab="Data and Fitted Values")
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20, col="red")
lines(LogisticModel.TighteningEasing_3M$fitted.values*20, col="green")
```

```{r}
LogisticModel.TighteningEasing_All <-
  glm(Tightening ~ .,
      data=AssignmentDataLogistic.EasingTighteningOnly[,c(1:7,10)],
      family=binomial(link=logit))
summary(LogisticModel.TighteningEasing_All)$aic
```

```{r}
summary(LogisticModel.TighteningEasing_All)$coefficients[,c(1,4)]
```

```{r}
matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)], type="l", ylab="Results of Logistic Regression")
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20, col="red")
lines(LogisticModel.TighteningEasing_All$fitted.values*20, col="green")
```

**Interpret the coefficients of the model and the fitted values.**

The logistic model on predicting tightening tries to use all of the treasury 
note date to predict whether the fed will be easiing or tightening interest 
rates. An easing period is when the FED is lowering interest rates while 
a tightening period is when the FED is raising interest rates.

In this model we are looking at all the data across all periods. The bright 
green line is the predicted easing/tightening probabiliyt. It's all over the 
map. While the model correctly gets the first and third tightening cycles,
it erroneously predicts two tightening cycles in the beginning where there
is none, and in the second real tightening cycle the predicted values 
drop off very quickly while the tightening actually continued for a brief 
period. This is not a very good model.

Looking at the coefficients we see why. They are warring with each other. and
the model is too sensitive to minor swings. The y-intercept says we start
in an easing cycle, which is true. Then all of the variables should be 
correlated in the same direction but they are not. The negative 3M coeffienct
means that as the 3M treasury note's interest rate rises, the model predicts
easing, not tightening. The same is true on the 3Y, 5Y, and 10Y notes. Meanwhile
the 6M, 2Y and 30Y inputs predict tightening as their yields rise. This doesn't
make a lot of sense and I think is evidence that the model is overfitted.

Calculate and plot log-odds and probabilities. Compare probabilities with fitted values.

```{r}
# Calculate odds
Log.Odds<-predict(LogisticModel.TighteningEasing_All)
plot(Log.Odds,type="l")
```

```{r}
Probabilities<-1/(exp(-Log.Odds)+1)
plot(LogisticModel.TighteningEasing_All$fitted.values, type="l", ylab="Fitted Values & Log-Odds")
lines(Probabilities,col="red")
```

# Step 5.

Compare linear regression models with different combinations of predictors.
Select the best combination.

Below we show only two of possible combinations: full model containing all 7 predictors and Null model containing only intercept, but none of the 7 predictors.
Estimate other possible combinations.

### Full Model

```{r}
AssignmentDataRegressionComparison <- 
  data.matrix(AssignmentData[,-c(9,10)], rownames.force="automatic")
AssignmentDataRegressionComparison <- AssignmentData[,-c(9,10)]
```

Estimate the full model by using all 7 predictors.

```{r}
fullModel <- lm(Output1~., data=AssignmentDataRegressionComparison)
fullModel.summary <- summary(fullModel)
```

Look at coefficients, R2, adjusted R2, degrees of freedom.

1. Coefficients:

```{r}
fullModel.summary$coef
```

2. R2, adjusted R2:

```{r}
fullModel.summary[c('r.squared','adj.r.squared')]
```

3. Degrees of freedom:

```{r}
fullModel.summary$df
```

**Intepret the fitted model. How good is the fit? How significant are the parameters?**

The fitted model with all seven predictors is perfectly fitted. Both the R^2 
and adjusted R^2 are 1. This means that 100% of the variance is explained by 
the predictors. All of the parameters are highly significant. 

Output1 was probably created by combining these measures, or the measures were
created from the Output1 variable itself.

### Intercept Only

Estimate the Null model by including only intercept.

```{r}
nullModel <- lm(Output1 ~ 1, data=AssignmentDataRegressionComparison)
nullModel.summary <- summary(nullModel)
(nullModel.summary$coefficients)
(nullModel.summary[c('r.squared','adj.r.squared')])
(nullModel.summary$df)
```

**Why does summary(nullModel) not show R^2?**

R^2 is not shown because there is no explained varianced in this model. R^2
is the ratio of explained to total variance. Total variance is the distance
of each predicted value from the mean. With no predictors there is no explained
variance and so the percentage is 0/total sum of squares.

Compare models pairwise using anova()

```{r}
anova(nullModel, fullModel)
```

**Interpret the results of anova().**

The results of the function `anova` on two models show if model 2 is "different" than
model 1 at a statisically significant level. That is, is the reduction in residual 
sum of squares due to random chance? Generally, adding more predictors will increase
the sum of squares explained, but does model 2 do this is a significant way?

The p-value here is basically 0. That is well below the threshold for significance.
Anova is saying that there is good evidence that adding all the predictors to a model
makes that model explain more of the variance than having no preditors in the model.

**Repeat the analysis for different combinations of input variables and select the one you think is the best.**

First I'll try to the first two predictors in the dataset: 3M & 6M

```{r}
model12  <- lm(Output1~., data=AssignmentDataRegressionComparison[,c(1,2)])
anova(nullModel, model12, fullModel)
```

The ANOVA results here show that after using the first two predictors, 14044 sum of 
squares remains unexplained. That's pretty good for starting with 637400 total sum
of squares. By the full model all of the variance is explained. What I want to look
for is a lot of explaination in that second row without adding a lot variables. Generally the fewer varables the better. You get almost all of the predictive power
with less computation and complication. I suspect that the 3M and 6M interest rates
are very tightly linked. It might be better to use more separation. 

Try with 3M and 30Y

```{r}
model17  <- lm(Output1~., data=AssignmentDataRegressionComparison[,c(1,7)])
anova(nullModel, model17, fullModel)
```

That's quite a bit better! Look at how little residual sum of squares if left after
only using two predictors. This is a significantly better model for only two 
predictors. 

I still think we can do better. Let's try using three non-adjacent predictors.

6M, 3Y, 10Y

```{r}
model246 <- lm(Output1~., data=AssignmentDataRegressionComparison[,c(2,4,6)])
anova(nullModel, model246, fullModel)
```

Wow this model is bascially as good as the full model with half of the predictors. 
This is probably the best model we're going to get. Let's try one more.

3M & 10Y

```{r}
model16  <- lm(Output1~., data=AssignmentDataRegressionComparison[,c(1,6)])
anova(nullModel, model16, fullModel)
```

Pretty darn good for only two predictors. 

**Explain your selection.**

I think that using the predictors 6M, 3Y and 10Y together yielded the best results. 
While the full model explained 100% of the variance, this model explains 99.99% of 
the results with half of the predictors. This makes a kind of intuitive sense since
it uses a short, medium, and long-term bond as the predictors then all of the bases
are covered.

My second pick would be using the 3M & 10Y predictors. This model explained 99.85% of 
the variance and at this point we're bascially splitting hairs. At only two predictors
this model is the best bang for your buck.

# Step 6.

Perform rolling window analysis of the yields data.
Use package zoo for rolling window analysis.

Set the window width and window shift parameters for rolling window.

```{r}
Window.width<-20
Window.shift<-5
```

Run rolling mean values usingrollapply().

```{r}
library(zoo)
```

Calculate rolling mean values for each variable.

```{r}
# Means
all.means <- rollapply(AssignmentDataRegressionComparison,  width=Window.width, by=Window.shift, by.column=TRUE, mean)
head(all.means,10)
```

```{r}
# Create points at which rolling means are calculated
Count <- 1:length(AssignmentDataRegressionComparison[,1])
Rolling.window.matrix <- rollapply(Count, width=Window.width, by=Window.shift, by.column=FALSE, FUN=function(z) z)
Rolling.window.matrix[1:10,]
```

```{r}
# Take middle of each window
Points.of.calculation <- Rolling.window.matrix[,10]
Points.of.calculation[1:10]
```

```{r}
length(Points.of.calculation)
```

```{r}
# Insert means into the total length vector to plot the rolling mean with the original data
Means.forPlot <- rep(NA, length(AssignmentDataRegressionComparison[,1]))
Means.forPlot[Points.of.calculation] <- all.means[,1]
Means.forPlot[1:50]
```

```{r}
# Assemble the matrix to plot the rolling means
cbind(AssignmentDataRegressionComparison[,1], Means.forPlot)[1:50,]
```

```{r}
plot(Means.forPlot, col="red")
lines(AssignmentDataRegressionComparison[,1])
```

Run rolling daily difference standard deviation of each variable

```{r}
df <- AssignmentDataRegressionComparison
DailyDifferencesList <- list()
for (i in 2:(nrow(df))) 
  DailyDifferencesList[[i-1]] <- data.frame(df[i+1,] - df[i,])
DailyDifferences <- plyr::ldply(DailyDifferencesList, data.frame)
```

```{r}
rolling.sd <- rollapply(DailyDifferences, width=Window.width, by=Window.shift, by.column=TRUE, sd)
head(rolling.sd)
```

```{r}
rolling.dates <- rollapply(AssignmentDataRegressionComparison[-1,], width=Window.width, by=Window.shift, by.column=FALSE, FUN=function(z) rownames(z))
head(rolling.dates)
```

```{r}
rownames(rolling.sd) <- rolling.dates[,10]
head(rolling.sd)
```

```{r}
matplot(rolling.sd[,c(1,5,7,8)], xaxt="n", type="l", col=c("black","red","blue","green"))
axis(side=1, at=1:1656, rownames(rolling.sd))
```

Show periods of high volatility. 

**How is volatility related to the level of rates?**

Volitility is spikiness, local spikiness. It's not related to overall total value
of interest rates. It's associated with either steep inclines or steep declines in rates. The biggest spikes in volitieis came on '81, '87, and '08 during marketn crashes. I'm a little suprised that 2001 isn't higher. All three of those periods
were local maximum in interest rates.

```{r}
# Show periods of high volatility
high.volatility.periods <- rownames(rolling.sd)[rolling.sd[,8]>.5]
high.volatility.periods
```

Fit linear model to rolling window data using 3 months, 5 years and 30 years variables as predictors.

```{r}
# Rolling lm coefficients
Coefficients <- rollapply(AssignmentDataRegressionComparison, 
                          width=Window.width, by=Window.shift, by.column=FALSE,
                          FUN=function(z) {
                            coef(lm(Output1 ~ USGG3M+USGG5YR+USGG30YR,
                                    data=as.data.frame(z)))
                          })
rolling.dates <- rollapply(AssignmentDataRegressionComparison[,1:8], width=Window.width, by=Window.shift, by.column=FALSE, FUN=function(z) rownames(z))
rownames(Coefficients) <- rolling.dates[,10]
Coefficients[1:10,]
```

Look at pairwise X-Y plots of regression coefficients for the 3M, 5Yr and 30Yr yields as inputs.

```{r}
# Pairs plot of Coefficients
pairs(Coefficients)
```

**Interpret the pairs plot.**

A pairs plot like this plots each variable against the others. It is used to 
visually compare interactions between predictors. Roughly, more linear shapes are 
more correlated while blobier shapes are less correlated.

The two most correlated predictors are the 5Y and 30Y rates. The pairs plot for those
show an almost straight line. The rest of the pairs I would call bascially 
uncorrelated. None of them particularly stand out, except for maybe 30Y and the 
intercept, but that's a bit of a stretch.

Finally, the direction of the relationship matters. The line formed by 30Y and 5Y has
a negative slope. This indicates these are inversely correlated, meaning that we would
expect the 30Y rate to rise as the 5Y fell and vice versa.

Plot the coefficients. Show periods.

```{r}
# Plot of coefficients
matplot(Coefficients[,-1], xaxt="n", type="l", col=c("black","red","green"))
axis(side=1, at=1:1657, rownames(Coefficients))
```

```{r}
high.slopespread.periods <-
  rownames(Coefficients)[Coefficients[,3]-Coefficients[,4]>3]
jump.slopes <- rownames(Coefficients)[Coefficients[,3]>3]
high.slopespread.periods
```

```{r}
jump.slopes
```

**Is the picture of coefficients consistent with the picture of pairs? If yes, explain why.**

Yes the picture of coefficients is consitent with the pairs plot. The pairs plot
shows that the 5Y and 30Y interest rates are inversely correlated. Here those rates
are in red and green and they appear to go in opposite directions. The huge spikes
in the red lines are accompanied by green spikes down below.

Meanwhile the black 3M line doesn't go with either the green or red. That is also
consistent with the pairs plot.

How often the R-squared is not considered high?

```{r}
# R-squared
r.squared.fun <- function(z) {
  summary(lm(Output1~USGG3M+USGG5YR+USGG30YR, data=as.data.frame(z)))$r.squared
}

r.squared <- rollapply(AssignmentDataRegressionComparison, 
                       width=Window.width, by=Window.shift, by.column=FALSE,
                       FUN=r.squared.fun)

r.squared <- cbind(rolling.dates[,10], r.squared)

r.squared[1:10,]
```

```{r}
plot(r.squared[,2],xaxt="n",ylim=c(0,1))
axis(side=1,at=1:1657,rownames(Coefficients))
```

```{r}
(low.r.squared.periods<-r.squared[r.squared[,2]<.9,1])
```

**What could cause decrease of R2?**

## TODO

Analyze the rolling p-values.

```{r}
# P-values
Pvalues <- rollapply(AssignmentDataRegressionComparison, width=Window.width, by=Window.shift, by.column=FALSE, FUN=function(z)
  summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$coefficients[,4])
rownames(Pvalues) <- rolling.dates[,10]
Pvalues[1:10,]
```


```{r}
matplot(Pvalues, xaxt="n", col=c("black","blue","red","green"), type="o")
axis(side=1,at=1:1657, rownames(Coefficients))
```


```{r}
rownames(Pvalues)[Pvalues[,2] > 0.5]
```


```{r}
rownames(Pvalues)[Pvalues[,3] > 0.5]
```


```{r}
rownames(Pvalues)[Pvalues[,4] > 0.5]
```

**Interpret the plot.**

## TODO

# Step 7.

Perform PCA with the inputs (columns 1-7).

```{r}
AssignmentData.Output <- AssignmentData$Output1
AssignmentData <- data.matrix(AssignmentData[,1:7], rownames.force="automatic")
dim(AssignmentData)
```

```{r}
head(AssignmentData)
```

Explore the dimensionality of the set of 3M, 2Y and 5Y yields.

```{r}
# Select 3 variables. Explore dimensionality and correlation 
AssignmentData.3M_2Y_5Y <- AssignmentData[,c(1,3,5)]
pairs(AssignmentData.3M_2Y_5Y)
```

Observe the 3D plot of the set. Use library rgl:

```{r}
library(rgl)
rgl.points(AssignmentData.3M_2Y_5Y)
```

Analyze the covariance matrix of the data. Compare results of manual calculation and `cov()`

```{r}
# Covariance of X & Y is sum((X-Xbar)*(Y-Ybar))/(n-1)
Centered <- apply(AssignmentData, 2, function(col) col-mean(col))
Manual.Covariance.Matrix <- 
  apply(Centered, 2, function(col) {
    apply(Centered, 2, function(col2) {
      sum(col * col2)/(nrow(AssignmentData)-1)
    })
  })
Manual.Covariance.Matrix
```


```{r}
Covariance.Matrix <- cov(AssignmentData) 
Covariance.Matrix
```

Plot the covariance matrix.

```{r}
Maturities<-c(.25,.5,2,3,5,10,30)
contour(Maturities,Maturities,Covariance.Matrix)
```

Perform the PCA by manually calculating factors, loadings and analyzing the importance of factors.

Find eigenvalues and eigenvectors. Calculate vector of means (zero loading), first 3 loadings and 3 factors.

```{r}
Eigen.Decomposition = eigen(Covariance.Matrix)
Eigen.Vectors = Eigen.Decomposition$vectors
Eigen.Values  = Eigen.Decomposition$values

Loadings = Eigen.Vectors
Factors = Centered %*% Eigen.Vectors
L0 = apply(AssignmentData, 2, function(col) mean(col))

Loadings = Loadings[,1:3]
Factors = Factors[,1:3]
```


```{r}
PCA = princomp(AssignmentData)
```


```{r}
barplot(Eigen.Values/sum(Eigen.Values), width=2, col = "black", names.arg=c("F1","F2","F3","F4","F5","F6","F7"))
```


```{r}
matplot(Maturities, Loadings, type="l", lty=1, col=c("black","red","green"), lwd=3)
```

**Interpret the factors by looking at the shapes of the loadings.**

## TODO

Calculate and plot 3 selected factors

```{r}
matplot(Factors, type="l", col=c("black","red","green"), lty=1, lwd=3)
```

Change the signs of the first factor and the corresponding factor loading.

```{r}
Loadings[,1] <- -Loadings[,1]
Factors[,1] <- -Factors[,1]
matplot(Factors, type="l", col=c("black","red","green"), lty=1, lwd=3)
```


```{r}
matplot(Maturities, Loadings, type="l", lty=1, col=c("black","red","green"), lwd=3)
```


```{r}
plot(Factors[,1], Factors[,2], type="l", lwd=2)
```

**Draw at least three conclusions from the plot of the first two factors above.**

## TODO

Analyze the adjustments that each factor makes to the term curve.

```{r}
OldCurve <- AssignmentData[135,]
NewCurve <- AssignmentData[136,]
CurveChange <- NewCurve - OldCurve
FactorsChange <- Factors[136,] - Factors[135,]
ModelCurveAdjustment.1Factor <- OldCurve+t(Loadings[,1])*FactorsChange[1]
ModelCurveAdjustment.2Factors <-
  OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]
ModelCurveAdjustment.3Factors <- 
  OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]+
  t(Loadings[,3])*FactorsChange[3]
matplot(Maturities, 
        t(rbind(OldCurve,NewCurve,ModelCurveAdjustment.1Factor,ModelCurveAdjustment.2Factors, ModelCurveAdjustment.3Factors)),
        type="l",
        lty=c(1,1,2,2,2),
        col=c("black","red","green","blue","magenta"),
        lwd=3,
        ylab="Curve Adjustment")
legend(x="topright", 
       c("Old Curve","New Curve","1-Factor Adj.", "2-Factor Adj.", "3-Factor Adj."),
       lty=c(1,1,2,2,2), 
       lwd=3, 
       col=c("black","red","green","blue","magenta"))
```


```{r}
rbind(CurveChange, ModelCurveAdjustment.3Factors - OldCurve)
```

**Explain how shapes of the loadings affect the adjustnents using only factor 1, factors 1 and 2, and all 3 factors.**

## TODO

See the goodness of fit for the example of 10Y yield.

```{r}
cbind(Maturities,Loadings)
```


```{r}
Model.10Y <- L0[6] + Loadings[6,1] * Factors[,1]+Loadings[6,2] * Factors[,2] + Loadings[6,3] * Factors[,3]
matplot(cbind(AssignmentData[,6], Model.10Y), type="l", lty=1, lwd=c(3,1), col=c("black","red"), ylab="5Y Yield")
```


```{r}
PCA.Yields <- princomp(AssignmentData)
names(PCA.Yields)
```

Compare the loadings.

```{r}
# Check that the loadings are the same
cbind(PCA.Yields$loadings[,1:3], Maturities,Loadings)
```


```{r}
matplot(Maturities, PCA.Yields$loadings[,1:3], type="l", col=c("black","red","green"), lty=1, lwd=3)
```


```{r}
matplot(PCA.Yields$scores[,1:3], type="l", col=c("black","red","green"), lwd=3, lty=1)
```

Change the signs of the first factor and factor loading again.

```{r}
# Change the signs of the 1st factor and the first loading
PCA.Yields$loadings[,1] <- -PCA.Yields$loadings[,1]
PCA.Yields$scores[,1] <- -PCA.Yields$scores[,1]
matplot(Maturities,PCA.Yields$loadings[,1:3], type="l", col=c("black","red","green"), lty=1, lwd=3)
```


```{r}
matplot(PCA.Yields$scores[,1:3], type="l", col=c("black","red","green"), lwd=3, lty=1)
```

Uncover the mystery of the Output in column 8.

```{r}
# What variable we had as Output?
matplot(cbind(PCA.Yields$scores[,1], AssignmentData.Output,Factors[,1]), type="l", col=c("black","red","green"), lwd=c(3,2,1), lty=c(1,2,3), ylab="Factor 1")
```

Compare the regression coefficients from Step 2 and Step 3 with factor loadings.

First, look at the slopes for `AssignmentData.Input~AssignmentData.Output`

```{r}
t(apply(AssignmentData, 2, function(AssignmentData.col) lm(AssignmentData.col~AssignmentData.Output)$coef))
```


```{r}
cbind(PCA.Yields$center, PCA.Yields$loadings[,1])
```

This shows that the zero loading equals the vector of intercepts of models `Y~Output1`, where Y is one of the columns of yields in the data.
Also, the slopes of the same models are equal to the first loading.

Check if the same is true in the opposite direction: is there a correspondence between the coefficients of models Output1~Yield and the first loading.

```{r}
AssignmentData.Centered<-t(apply(AssignmentData, 1, function(AssignmentData.row)
  AssignmentData.row-PCA.Yields$center))
dim(AssignmentData.Centered)
```


```{r}
t(apply(AssignmentData.Centered, 2, function(AssignmentData.col) lm(AssignmentData.Output~AssignmentData.col)$coef))
```

To recover the loading of the first factor by doing regression, use all inputs together.

```{r}
t(lm(AssignmentData.Output~AssignmentData.Centered)$coef)[-1]
```


```{r}
PCA.Yields$loadings[,1]
```

This means that the factor is a portfolio of all input variables with weights.

```{r}
PCA.Yields$loadings[,1]
```
